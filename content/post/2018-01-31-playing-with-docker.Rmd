---
title: "Docker Basics: an Overview and Example"
author: Jonathan Lamberts
date: '2018-02-13'
slug: docker-basics
categories:
  - Jonathan Learns a Thing
  - Examples
tags:
  - Docker
  - OpenCV
  - Computer Vision
  - Jupyter
draft: false
---

# A Little Bit of Background
This semester, I'm taking a course on Computer Vision as part of my Master's degree at Georgia Tech.  The course depends heavily on a specific version of the Python [OpenCV library](https://opencv.org/).  As a conda guy, I first tried creating a new conda environment and using that for my assignments.

Unfortunately, it turns out that, while OpenCV is available on [Conda Forge](https://anaconda.org/conda-forge/opencv), only the Ubuntu version of the package hosted there works properly with FFmpeg.  This means that a purely conda-based environment is unable to read and write video on Windows.  Rather than dealing with configuring my environment on each of my individual machines, I decided to learn something while I fixed the problem and see if I could make a platform-agnostic version of the environment using conda and Docker.  

Ultimately, I ended up creating two images: one contains the base requirements for running and submitting assignment code; the other runs a Jupyter notebook to allow for interactive experimentation when working on assignments (turns out that a lot of Computer Vision is turning knobs and seeing what happens).  In this post I'll give an overview of what I learned about Docker and what tripped me up, in the hope that it will help someone else in a similar situation.  I'll also use the images I created as examples to explain some Docker concepts.

# Learning the Docker Basics
When I started, I only had a vague notion that Docker would allow me to keep my library versions consistent across machines and operating systems; I didn't really know anything about the how Docker works.  Here's a brief overview.

## The Docker Workflow
Beginning with writing the specifications for an environment and ending with a fully functioning container that actually runs code in that environment, the docker workflow has 3 main components.  A **Dockerfile** contains the specifications necessary to build an environment.  An **Image** contains a snapshot of a system that is created using the dockerfile.  A **Container** is a specific instance of an image that actually executes your code or lets you interact with the image's environment (e.g. with a bash shell).  Docker also provides the **Docker Hub**, a public registry of images which can be run directly or used as the basis for a new custom image.^[It's also possible to run your own registry of images.  Docker Hub is just the public one provided by Docker.]  See my beautiful diagram below.

![*A simplified version of Docker's workflow*](/img/Docker Flow.png)

## The Dockerfile: Building the Image
Essentially, a Dockerfile contains a list of commands that run to create an image and set up that image's environment.  In my case,  those commands install the required version of the necessary libraries, and set things up so that I can use a jupyter notebook on my host machine while the notebook runs inside of the container.  Let's take a look at the Jupyter image's dockerfile and break down its parts.

```
FROM jlamberts/omscs-cv-base:spring2018
MAINTAINER Jonathan Lamberts <jonathan.lamberts@gmail.com>

# Install Matplotlib and Jupyter for interaction
RUN conda install -c conda-forge opencv=2.4 matplotlib jupyter

# Expose the notebook port
EXPOSE 8888

RUN mkdir /usr/sandbox/

ENTRYPOINT ["python", "/opt/conda/bin/jupyter-notebook", "--ip=*", "--allow-root"]
CMD ["--notebook-dir=/usr/sandbox"]
```
First off, you'll notice that I'm loading from another image:   

`FROM jlamberts/omscs-cv-base:spring2018`  

The `FROM` instruction tells docker to load another image and then execute the rest of the commands on top of that image.  Remember that an image is just a snapshot, so this essentially unfreezes the snapshot and installs more stuff on top of it.  One particularly nice feature of Docker is that if you load from an image that you don't have locally, it will check on the [Docker Hub](https://hub.docker.com/), pull it from there, and cache it.  In this case, I'm pulling my base image which already has most of the requirements installed; that base image is in turn derived from ContinuumIO's miniconda image.

Next, the line 

`RUN conda install -c conda-forge opencv=2.4 matplotlib jupyter` 

uses conda to install Matplotlib, Jupyter, and OpenCV^[Technically since openCV is already installed on the base image, it doesn't actually install here, but including it in the command along with its version prevents it from upgrading and violating the environment requirements].  In general, the `RUN` instruction simply runs a specified command in a shell (bash by default on Linux).

As you may have gathered from the comment in the dockerfile, the `EXPOSE 8888` instruction exposes port 8888, used by the notebook, to the host machine.  Since a docker container runs in a VM, there's no network access between the host machine and the container by default.

Finally, the `ENTRYPOINT` and `CMD` instructions tell the image what to do when it is actually run.  Specifically, `ENTRYPOINT` lists commands that are **always**^[You can actually override it with `--entrypoint` flag, which I found helpful when debugging] run when the image is run, and the `CMD` instruction provides **default** arguments.  The `CMD` arguments are appended to the `ENTRYPOINT` arguments unless the user provides any arguments themselves.  So

`docker run jlamberts/omscs-cv-jupyter`

runs the notebook with /usr/sandbox as the base directory, as specified by `CMD`.  Running with an additional argument overrides the `CMD`, so 

`docker run jlamberts/omscs-cv-jupyter --notebook-dir=/usr` 

runs the notebook with the `--notebook-dir=/usr` argument, and the notebook runs in the /usr directory instead.  Either way, the `ENTRYPOINT` commands are executed.

## The Image: Building The Environment
Now that we have our dockerfile, we can use it to create an image.  When we run the command 

`docker build -t omscs-cv-jupyter .` 

in the directory with our dockerfile, Docker will parse the file, pull the necessary dependencies, run the commands in the dockerfile, freeze the final image, and tag it (that's what the `-t` stands for) with the name omscs-cv-jupyter.  After this runs, you can type 

`docker images` 

and you'll see that it's included in the list of images available on your machine: 

![](/img/docker_images.png)

You might notice that several other images and repositories show up in the list.  Whenever you load `FROM` an image in a dockerfile, docker checks locally for that image, and pulls it from Docker Hub if it can't find it.  It then caches the image on your system so that you can keep using it without needing to redownload it.  In my case, the omscs-cv-jupyter image requires omscs-cv-base, which in turn requires continuum/miniconda. 

So what if you want to use your image on other machines? That was the whole reason I wanted to use Docker in the first place, after all.  Once the image is built, it's fairly straightforward to push it to a public repository so you can use it wherever you want.  Assuming you already have an account at [Docker Cloud](https://cloud.docker.com),

`docker login` 

will allow you to log in to your account.  From there, 

`docker tag omscs-cv-jupyter jlamberts/omscs-cv-jupyter:latest`  

will associate the omscs-cv-jupyter image we just built with the jlamberts/omscs-cv-jupyter repository, tagged as *latest*. Finally,

`docker push jlamberts/omscs-cv-jupyter:latest`  

will push the tagged image to a public repository on Docker Hub.

## Running a Container
Once the image is built, running it locally is pretty straightforward.  `docker run [image_name]` will check locally for an image.  If it finds the image, it will create and run a container based on that image; if not, it will check Docker Hub and automatically download it before creating the container.

One important thing to know about Docker containers is that they are transient: they are designed to be created and destroyed and nothing that happens inside of a container affects the base image.  This allows for convenient sandbox environments, as well as the ability to quickly scale services by creating multiple containers from a single image and load-balancing between them.  Since the containers are isolated from the host machine, they're also portable across environments and operating systems.


In my specific case, I'm more interested in containers' portability than scalability.  To use a notebook in my CV environment, I run

`docker run -it --rm -p 8888:8888 -v /c/users/jlamberts/documents/ps02:/usr/sandbox/ps02 jlamberts/omscs-cv-jupyter`

This has a few additional arguments passed to `docker run`, so I'll explain each one.

* The `-it` flag tells the container to run **i**nteractively and to allocate a **t**ty, passing messages between the container and my terminal.^[These can also be as seperate `-i` and `-t` flags.]
* The `--rm` flag cleans up after the container finishes running, removing its filesystem and preventing a bunch of old containers from sticking around taking up space.
* `-p 8888:8888` maps the container's port 8888 (the default Jupyter Notebook port) to my local port 8888, allowing me to open the notebook server in my web browser at localhost:8888.
* the `-v` flag mounts a **v**olume on the host machine to one in the container.  In this case, my C:\\\\users\\jlamberts\\documents\\ps02 directory, containing a homework assignment, maps to the container's /usr/sandbox/ps02 directory, which is where the notebook runs by default.  This means I'll be able to save my work.

The final result: a Jupyter notebook server which runs inside of a container and gives me access to the OMSCS CV environment and allows me to use it with local files.
![](/img/docker_jupyter.png)

# An Issue and its Solution
I ran into some issues when trying to mount the volume of my local machine to the docker image on windows.  The container would start successfully, but the mounted directory would show up as empty in the container.  The solution ended up being stupidly simple: I just had to share the drive in the docker settings.  
![sharing a drive](/img/docker_share_drive.png)
If you're having a similar issue and this doesn't fix it, I also found a [github issue](https://github.com/docker/for-win/issues/77) reporting that people were able to solve the problem by unsharing then sharing access to the drive.

# Conclusion
In the end, this docker setup worked great and let me use OpenCV the way that I needed to for my assignment.  After getting over the initial hurdle of understanding how to write a dockerfile, I think containers could potentially be really useful for some of the one-off analyses I tend to do in notebooks.  I'll probably experiment some more with this and will write a follow-up post if I find any neat tricks.  In the meantime, feel free to shoot me an email if you have any questions or comments.

# Links and Resources
* [My repo with the dockerfiles for each image](https://github.com/jlamberts/OMSCS-CV-Docker)
* [The omscs-cv-base image on Docker Hub](https://hub.docker.com/r/jlamberts/omscs-cv-base/)
* [The omscs-cv-jupyter image on Docker Hub](https://hub.docker.com/r/jlamberts/omscs-cv-jupyter/)
* Docker's [official getting started guide](https://docs.docker.com/get-started/) is a nice, readable intro to Docker
